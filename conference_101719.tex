\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}   
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Transformer Based Pipeline for Software Requirements Classification\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Maheen Mashrur Hoque}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{Islamic University of Technology}\\
Gazipur, Bangladesh \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Arman Hossain Dipu Khan}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{Islamic University of Technology}\\
Gazipur, Bangladesh \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Ahsan Habib}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{Islamic University of Technology}\\
Gazipur, Bangladesh \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Ajwad Abrar}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{Islamic University of Technology}\\
Gazipur, Bangladesh \\
email address or ORCID}
\and
}

\maketitle

\begin{abstract}
The automation of the software development lifecycle (SDLC) is a key challenge in the field of software development. One of the main challenges in automation of the SDLC lies within the work of collecting, analyzing and establishing requirements, where the requirements collected from the stakeholders are often noisy, which can be difficult to organize. In order to facilitate the automation of requirement analysis, our study proposes a cost-effective approach - leveraging the lightweight DistilBERT and RoBERTA models with a method called ``Ensemble Pooling'' to filter out relevant requirements. Our experiments showcased an accuracy of 78\% for the ``Ensemble Pooling'' method, a higher score than readily available sequence classifier versions of the aforementioned models. 

\end{abstract}

\begin{IEEEkeywords}
Software Requirements, Software Development Lifecycle (SDLC), Classification, Natural Language Processing (NLP), Transformers,Ensemble 
\end{IEEEkeywords}

\section{Introduction}
Software requirements can be thought of very high level description of what a software system is expected to do in order to solve a business requirements and are the reflection of the expectations of the stakeholders and clients \cite{pohl2016requirements}. A requirement should have clarity, be consistent and have completeness \cite{wiegers2013software}. However, factors like communication gap, difference in perspective, and complexity of scope can make requirements vague and difficult to comprehend for the developer \cite{nuseibeh2000requirements}. Due to these factors, requirements from the stakeholders may often contain irrelevant information (which we are referring to as ``noise''), which can later on cause difficulties to analyze those requirements (for example, discerning functional and non-functional requirements). This causes bottleneck in the SDLC as system architects spend much time organizing the requirements. The challenge of requirements organization can be classified as a natural language processing (NLP) problem, with the process of filtering actual requirements from the irrelevant texts being the first step. Low infrastructural cost is also a factor of consideration here. So, our research objective in this work can be defined as - ``Noise reduction via classifying relevant and irrelevant requirements, through cost-effective means'', with the works of Ivanov et. al. \cite{ivanov2021regexppure_extracting_software_requirements} acting as baseline. The experiments, dataset and results are discussed in later sections.


\section{Literature Review}
\label{Literature Review}

This section provides a comprehensive descriptions of the previous works that were reviewed for this research. In order to avoid parochial view over the research objective, the reviews were not limited to only language model based approaches.

\subsection{Esoteric Approach}

The works of  Siahaan and Darnoto distinguishes irrelevant requirements using the MultiPhiLDA  method \cite{siahaan2022novel}. Their method works by distinguishing topic–word distribution of actor words and action words, governed by polynomial probability functions, essentially making it a statistical distribution analysis. 

Another method by Alharbi et. al. leverages semantic embeddings to filter ambiguous requirements \cite{alharbi2022ambiguity}. The embedding were fine-tuned to the specific task, allowing it to capture context-specific nuances related to ambiguity.

Similarly, the works pf Malik et. al. leverages cosine similarity over sentence embeddings to classify conflicting requirements \cite{malik2022identifying}. Although, not directly related, this work provided us insight on how to levarage similarity scores.

Another approach by Norabid et. al., is to define Entity-Relation Extraction Rules for performing linguistic analysis to extract dependency relations and part-of-speech (POS) in formation from a given text \cite{norabid2022rule}. These rules guide the extraction of entities and relations from the text. Final step is Triple Extraction and MKG Construction. A triple extractor is developed, incorporating the rules. The extractor extracts triples (subject-relation-object) from the news articles dataset. These triples form the basis of this MKG based study of the authors.

\subsection{Language Model Approach}

A language model based approach by Ivanov et. al. for extracting requirements from unstructured text uses BERT \cite{devlin2018bert} model fine-tuned with a manually annotated dataset from the PURE (Public Requirements) corpus \cite{ferrari2017pure}. They achieved an accuracy of 74\%, compared to two other baseline approaches -- fastText (open source natural language processing toolkit) with an accuracy of 60\%, and ELMo (a foundational text-embedding model) paired with SVM (Support Vector Machine classifier) with an accuracy of 59\%. Due to the similarity of objective, this work was considered as a baseline for us to compare against. 


\section{Dataset}
\label{Dataset}

For our work the, the PURE (Public Requirements) dataset \cite{ferrari2017pure}, created by Ferrari et. al., was the primary source of data for the experiments of RO1. This contains 79 software requirements document (SRS) that contains 34,286 sentences. However, the dataset does not provide any additional labelling for the requirements sentences to aid natural language processing tasks. Some of the previous works, notably \cite{ivanov2021regexppure_extracting_software_requirements} and \cite{siahaan2022novel} utilized this dataset in their works via labeling in various ways. 

A labeled subset of the PURE dataset, called the RegExpPURE Dataset by Ivanov et. al. \cite{ivanov2021regexppure_extracting_software_requirements} was used in our studies. This dataset has proper balancing in it's data, with 2474 not requirements and 2832 requirements in train, 467 not requirements and 1058 requirements in test and 255 not requirements and 650 requirements in validation set, making it a suitable training candidate. Note that these same train, test, and validation sets were used to get a more comparable experiment setup to compare with the baseline work. 


\section{Methodology}
\label{Methodology}

From the discussions of possible approaches for NLP tasks concerned with primarily extraction and classification presented in \autoref{Literature Review}, it was decided to approach by leveraging transformers models. Note that much of our approach is uni-modal, only working with text based data. As of our current advancement and the nature of how requirements descriptions usually delivered to the development personnel and stakeholders, it is a reasonable assumption that utilizing text based data would be sufficient. The future extension of our work may be benefited from using a multimodal approach, similar to the works of Norabid et. al. \cite{norabid2022rule}.

\subsection{Choice of Language Models}

Here, we utilized two lightweight language models, based on the Transformers architecture - DistilBERT and RoBERTa. 

For natural languages, it is quite common that the correlation amongst the words in the context of the expression of that sentence can showcase long range dependencies. The self-attention mechanism allows Transformer models to capture long-range dependencies more effectively than RNNs and LSTMs, which struggle with long-term context due to their sequential nature \cite{vaswani2017attention_is_all_you_need}. Moreover the architecture of Transformer models is highly versatile and can be adapted to a wide range of NLP tasks, including the classification task in this work.

DistilBERT is a lightweight variant of the BERT model which retains almost 90\% of the BERT's performance in various NLP benchmarks \cite{sanh2019distilbert}. DistilBERT uses model compression (student-teacher approach) to reduce layers from BERT to half while matching the logits, hidden states and attention distribution. The reason why we chose this model for our work is the efficiency that DistilBERT showcases over BERT. DistilBERT requires lower memory, which makes it ideal for anyone to utilize this approach of this study without any significant computing requirements. On the other hand, RoBERTa is an optimized version of BERT that is trained on a larger text corpus than that of BERT and also uses dynamic masking during pre-training, which changes the masking pattern of the input sequence at each epoch, allowing the model to see a larger variety of contexts \cite{liu2019roberta}. Moreover, RoBERTa retains the same architecture as BERT but optimizes hyperparameters, including removing the Next Sentence Prediction (NSP) objective. This modification improves the efficiency of the training process and the model's overall performance \cite{liu2019roberta}. Due to these reasons, RoBRETa outperforms BERT outperforms BERT in many benchmarks and is a strong yet lightweight candidate for classification tasks. Hence, it was also chosen in our experiment. 

\subsection{Definition of Noise in Requirements}

In the context of software requirements, noise can be referred to as irrelevant information or non-essential details that do not contribute to the actual functionality or goals of the system. These noises can be redundant information, ambiguous language or non-requirements. To illustrate an example from the RegExpPURE dataset \cite{ivanov2021regexppure_extracting_software_requirements}, the following is a clear requirement:
\begin{quote}
    System Initialization shall [SRS014] initiate the watchdog timer.
\end{quote}
And the following is a noise:
\begin{quote}
    A Working Group was established in May, 2006 to develop high-level functional specifications for an NLM Digital Repository for NLM collection materials and to identify policy and management issues related to the creation, design and management of the repository.
\end{quote}
We can see that in order to recognize what is a requirement and what is not a requirement, one needs to read the entire sentence and map the terms of the sentences to clarify what the sentence expresses. This can be used as an analogy of what the term `long range dependency' entails for transformer models, the model's awareness of the expressive meaning of every term in the sequence with one another. 

\subsection{Ensemble Pooling Approach}

For this classification task of filtering noise, we propose a method that we call ``Ensemble Pooling''. Our approach consists of three layers - 
\begin{enumerate}
    \item \textbf{Tokenizer Layer :} This layer will appropriately tokenize the input sequence from text to corresponding vector representation.
    \item \textbf{Transformer Layer :} This layer will produce the contextual embedding for the input sequence by feeding into the vector representation of the tokenizer layer.
    \item \textbf{Classifier Layer :} This layer will use the contextual embedding (via pooling) produced by the transformer layer to classify the requirements from the noise. This layer would be trained with appropriate labels for classification. In our experimentation, we got better result with ensemble classifiers (hence the name, `Ensemble Pooling'). 
\end{enumerate}

The following \autoref{fig:ensemble pooling classification approach} represents the architecture of our approach.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{Figures/ensemble_pooling_classification.png}}
\caption{The Ensemble Pooling Classification Approach}
\label{fig:ensemble pooling classification approach}
\end{figure}

Here, we utilized the last hidden layer of the transformer models, which contains integrated information or the final contextualized embeddings from the entire sequence in a multidimensional vector representation (in the Huggingface Transformer library \cite{sanh2019distilbert}, the dimension is ($Seq_{size} \times Seq_{length} \times Size_{hidden}$) the default hidden size is $768$). But the most important content of the layer is the [CLS] token representation, which is added to each of the embeddings vector by the tokenizer and the vector for that token acts as the aggregate representation of the entire sequence \cite{devlin2018bert} \cite{sarzynska2021detecting_thought_contextualized}, that we use with the ensemble classifiers for classification tasks. The DistilBertForSequenceClassification and RobertaForSequenceClassification (based on DistilBERT and RoBERTa) classifiers offered by the Transformers library uses a similar approach, but with linear classification layer on top. These were compared as baselines.

Note that the transformer layer does not have any training cycle. This is because the transformer models used here (BERT and RoBERTa) are already trained over a large corpus of the English language and already have the necessary vocabulary structuring for language related tasks \cite{sanh2019distilbert} \cite{liu2019roberta}. This facilitates in reducing the cost and time that one had to invest with training. 

\subsection{Classification Layer}

The classification layers were trained over the pooled vectors from the transformer layer and the labels from the dataset. The labels were converted to 0 for not a requirement (i.e. noise) and 1 for requirement (i.e a proper requirement). Then 4 classifier models were trained and evaluated, which were Random Forest Classifier, XGB Classifier, LightGBM classifier and SVM Classifier. Out of these, 3 are ensemble models (Random Forest, XGB, LightGBM). We chose ensemble models because they leverage multiple learning algorithms (or sub-classifiers) to obtain better predictive performance than could be obtained from any of the standalone models. Moreover, due to combining sub-classifier, ensemble models are robust against overfitting \cite{breiman2001randomforest}. On the other hand, SVM is not an ensemble model, rather it is a standalone classifier that determines a hyperplane to best separate the datapoints into classes \cite{cortes1995svm}. SVM was also experimented with as it had shown good performance in NLP related classification tasks in several works \cite{drucker1999svm_spam} \cite{torisawa2007svm_ner} \cite{torisawa2007svm_ner}.

\subsection{Training Parameters}

For the experiments, the core DistilBERT and RoBERTa models, used for ensemble pooling were not trained specifically for this study. However the two transformer based classifiers and the other classifiers were trained. The transformer based classifier models were fine tuned for classification, with the hyperparameters being the same for both - Batch Size = 16, Max Length = 128, Learning Rate = $2\times 10^{-5}$ and was trained over 10 epochs. Adam optimizer was used and Cross Entropy Loss was the loss function for both of the models. Both DistilBERT and RoBERTa is based on the BERT model. Their parameters have no difference. Where they do differ is in how they handle sequences in their inner layers (which is discussed in \autoref{Methodology})

For the classifiers, all were fine tuned with 10 fold grid search cross validation. The target metric for the hyperparameter tuning was accuracy. The optimal hyperparameters for the models are as follows in \autoref{tab:lgbm_hyperparameters}, \autoref{tab:rf_hyperparameters}, \autoref{tab:svm_hyperparameters}, and \autoref{tab:xgb_hyperparameters}.

\begin{table}[htbp]
\caption{Random Forest Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{ } & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
Max Depth (\texttt{max\_depth})            & None & 10 \\
\hline
Min Samples Leaf (\texttt{min\_samples\_leaf})  & 4 & 4 \\
\hline
Min Samples Split (\texttt{min\_samples\_split}) & 2 & 2 \\
\hline
Number of Estimators (\texttt{n\_estimators})   & 300 & 100 \\
\hline
\end{tabular}
\label{tab:rf_hyperparameters}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{LightGBM Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{} & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
Colsample by Tree (\texttt{colsample\_bytree})   & 1.0 & 1.0 \\
\hline
Learning Rate (\texttt{learning\_rate})          & 0.1 & 0.1 \\
\hline
Min Child Samples (\texttt{min\_child\_samples}) & 20  & 50 \\
\hline
Number of Estimators (\texttt{n\_estimators})    & 300 & 200 \\
\hline
Num Leaves (\texttt{num\_leaves})                & 50  & 50 \\
\hline
Subsample (\texttt{subsample})                   & 0.8 & 0.8 \\
\hline
\end{tabular}
\label{tab:lgbm_hyperparameters}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{XGBoost Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{} & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
Learning Rate (\texttt{learning\_rate}) & 0.1 & 0.1 \\
\hline
Max Depth (\texttt{max\_depth}) & 5 & 5 \\
\hline
Number of Estimators (\texttt{n\_estimators}) & 300 & 300 \\
\hline
Subsample (\texttt{subsample}) & 0.8 & 0.7 \\
\hline
Colsample by Tree (\texttt{colsample\_bytree}) & 0.8 & 1 \\
\hline
Gamma (\texttt{gamma}) & 0.1 & 0 \\
\hline
\end{tabular}
\label{tab:xgb_hyperparameters}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{SVM Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{} & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
C (\texttt{C}) & 10 & 10 \\
\hline
Gamma (\texttt{gamma}) & scale & auto \\
\hline
Kernel (\texttt{kernel}) & rbf & rbf \\
\hline
\end{tabular}
\label{tab:svm_hyperparameters}
\end{center}
\end{table}

Again, the term ``Pairing'' is referred to as using the hidden layer of the language models through the classifier models to make predictions. 


\section{Results}
\label{Results}

As per the setup described in \autoref{Methodology}, we compared our experiments with the baseline paper. Similar to the baseline work, results are based on the validation set. For the result comparison, primarily used the accuracy metric (The rationale behind this choice of metric is discussed in the \autoref{Discussion}). The \autoref{table:results_comparison} contains the results and the \autoref{table:baseline_results} shows the baseline paper result.  

\begin{table}[htbp]
\caption{Results Comparison of the Various Approaches}
\begin{center}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Approach Category} & \textbf{Model / Technique} & \textbf{Accuracy (\%)} \\
\hline
\multirow{5}{*}{\textbf{\textit{DistilBERT}}}
 & DistilBERT Sequence Classifier & 76 \\
 & DistilBERT + Random Forest     & \textbf{78} \\
 & DistilBERT + XGB               & 77 \\
 & DistilBERT + LightGBM          & 77 \\
 & DistilBERT + SVM               & 76 \\
\hline
\multirow{5}{*}{\textbf{\textit{RoBERTa}}}
 & RoBERTa Sequence Classifier    & 77 \\
 & RoBERTa + Random Forest        & 75 \\
 & RoBERTa + XGB                  & 77 \\
 & RoBERTa + LightGBM             & 77 \\
 & RoBERTa + SVM                  & 77 \\
\hline
\end{tabular}
\label{table:results_comparison}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Results of the Baseline Paper \cite{ivanov2021regexppure_extracting_software_requirements}}
\begin{center}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|c|c|}
\hline
\textbf{Approach} & \textbf{Accuracy (\%)} \\
\hline
fastText   & 60 \\
\hline
ELMo + SVM & 59 \\
\hline
BERT       & 74 \\
\hline
\end{tabular}
\label{table:baseline_results}
\end{center}
\end{table}

For our experiments and proposed approach of identifying requirement noise, we used the RegExpPURE dataset \cite{ivanov2021regexppure_extracting_software_requirements}. So, our baseline was the results in that paper for the dataset in which they utilized the BERT base model for extracting requirements. Though their work may not directly state noise reduction, but their work achieved similar objectives. As seen in \autoref{Dataset} their dataset consists labeled sentences of requirements and non requirements. And these non requirement sentences are precisely what we have defined as noise. So, the rationale for using this paper as a baseline is quite reasonable.


\section{Discussion}
\label{Discussion}

\subsection{The Rationale for Accuracy}

Accuracy is intuitive and easy to understand. It directly reflects the overall correctness of the model’s predictions. In many scenarios, balanced performance across different classes is desirable. Accuracy treats all classes equally. Moreover, when the class distribution is roughly uniform (which is the case for the RegExpPURE Dataset), accuracy is a reasonable choice. And hence, accuracy was the chosen metric. \cite{sokolova2009systematic_classification}.

\subsection{Base Classifier vs Ensemble Pooling}

From the results in \autoref{table:results_comparison} and \autoref{table:baseline_results} shows that our approach combining DistilBERT and Random Forest in ensemble pooling performs better in terms of accuracy when compared against all the approaches and baseline results. However, all the approaches with RoBERTa had quite close results, except Random Forest. This deviation is most likely due to how RoBERTa utilizes byte-pair encoding in tokenization and how the inner encoder layers attune to the context of the sequences \cite{liu2019roberta}. 

The baseline paper showed the most promising result with BERT with about 74\% in accuracy. But baseline classifier of BERT, similar to DistilBERT and RoBERTa uses a neural network layer with a linear softmax activation function to generate class probability distribution. While effective in most cases, this classification task can benefit from more robust method of classification, ensemble classification. Since ensemble classifiers use bagging of sub classifiers, it is more robust in classification. Random Forest is one such ensemble method that ensembles decision trees. These trees or sub classifiers work in tandem for classification and improves accuracy, as seen from our experiments. Again, the rationale behind choosing DistilBERT and RoBERTa comes to their lightweight performance. They require much less computing resources compared to some other larger models and still remains a popular choice for smaller NLP related works. 

However, the lightweight nature is also one of the drawbacks of our approach. The accuracy results are still in the range below 80, which is a clear indication that these models fall short due to having smaller number of parameters compared to state of the art GPT, Gemini and LLAMA models (inferred from the ``Densing law of LLMs'' \cite{xiao2025densing}). So, as a future extension in requirement noise reduction, the viability of the state of the art models as well as stronger classification layers (stacking multiple classifiers for starters) remain as an yet to explore prospect. 


\section{Conclusion}
\label{Conclusion}

In conclusion, this study contributed valuable insights into the application of transformer models in natural language processing tasks, specifically in the context of software requirements classification. By focusing on noise reduction, this research lays the foundation for enhancing the efficiency and effectiveness of the software development process. Our developed methodology showcased improved performance when compared with various out-of-the-box classifiers in terms of accuracy.

While the current work primarily addressed noise reduction in requirements, the future direction of this research involves extending the classification pipeline to encompass all research objectives, including improving requirement assessment, conflict resolution and multimodal analysis (involving charts, diagrams, wireframes - etc.). 

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}



\bibliographystyle{IEEEtran}
\bibliography{Bibliography}


\end{document}
