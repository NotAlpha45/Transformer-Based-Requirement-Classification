%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

\usepackage{cite}   
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}



%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{A Transformer Based Pipeline for Software Requirements Classification}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Maheen Mashrur Hoque}
\affiliation{%
  \institution{Islamic University of Technology}
  \city{Gazipur}
  \country{Bangladesh}
  }
\email{maheenmashrur@iut-dhaka.edu}

\author{Arman Hossain Dipu}
\affiliation{%
  \institution{Islamic University of Technology}
  \city{Gazipur}
  \country{Bangladesh}
}
\email{armanhossain@iut-dhaka.edu}

\author{Ahsan Habib}
\affiliation{%
  \institution{Islamic University of Technology}
  \city{Gazipur}
  \country{Bangladesh}
}
\email{ahsanhabib@iut-dhaka.edu}

\author{Ajwad Abrar}
\affiliation{%
  \institution{Islamic University of Technology}
  \city{Gazipur}
  \country{Bangladesh}
}
\email{ajwadabrar@iut-dhaka.edu}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The automation of the software development lifecycle (SDLC) is a key challenge in the field of software development. One of the main challenges in automation of the SDLC lies within the work of collecting, analyzing and establishing requirements, where the requirements collected from the stakeholders are often noisy, which can be difficult to organize. In order to facilitate the automation of requirement analysis, our study proposes a cost-effective approach - leveraging the lightweight DistilBERT and RoBERTA models with a method called ``Ensemble Pooling'' to filter out relevant requirements. Our experiments showcased an accuracy of 78\% for the ``Ensemble Pooling'' method, a higher score than readily available sequence classifier versions of the aforementioned models. 

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010405.10010497.10010504.10010505</concept_id>
<concept_desc>Applied computing~Document analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Document analysis}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Software Requirements, Software Development Lifecycle (SDLC), Classification, Natural Language Processing (NLP), Transformers,Ensemble 
}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle



\section{Introduction}
Software requirements can be thought of very high level description of what a software system is expected to do in order to solve a business requirements and are the reflection of the expectations of the stakeholders and clients \cite{pohl2016requirements}. A requirement should have clarity, be consistent and have completeness \cite{wiegers2013software}. However, factors like communication gap, difference in perspective, and complexity of scope can make requirements vague and difficult to comprehend for the developer \cite{nuseibeh2000requirements}. Due to these factors, requirements from the stakeholders may often contain irrelevant information (which we are referring to as ``noise''), which can later on cause difficulties to analyze those requirements (for example, discerning functional and non-functional requirements). This causes bottleneck in the SDLC as system architects spend much time organizing the requirements. The challenge of requirements organization can be classified as a natural language processing (NLP) problem, with the process of filtering actual requirements from the irrelevant texts being the first step. Low infrastructural cost is also a factor of consideration here. So, our research objective in this work can be defined as - ``Noise reduction via classifying relevant and irrelevant requirements, through cost-effective means'', with the works of Ivanov et. al. \cite{ivanov2021regexppure_extracting_software_requirements} acting as baseline. The experiments, dataset and results are discussed in later sections.


\section{Literature Review}
\label{Literature Review}

This section provides a comprehensive descriptions of the previous works that were reviewed for this research. In order to avoid parochial view over the research objective, the reviews were not limited to only language model based approaches.

\subsection{Esoteric Approach}

The works of  Siahaan and Darnoto distinguishes irrelevant requirements using the MultiPhiLDA  method \cite{siahaan2022novel}. Their method works by distinguishing topic–word distribution of actor words and action words, governed by polynomial probability functions, essentially making it a statistical distribution analysis. 

Another method by Alharbi et. al. leverages semantic embeddings to filter ambiguous requirements \cite{alharbi2022ambiguity}. The embedding were fine-tuned to the specific task, allowing it to capture context-specific nuances related to ambiguity.

Similarly, the works pf Malik et. al. leverages cosine similarity over sentence embeddings to classify conflicting requirements \cite{malik2022identifying}. Although, not directly related, this work provided us insight on how to levarage similarity scores.

Another approach by Norabid et. al., is to define Entity-Relation Extraction Rules for performing linguistic analysis to extract dependency relations and part-of-speech (POS) in formation from a given text \cite{norabid2022rule}. These rules guide the extraction of entities and relations from the text. Final step is Triple Extraction and MKG Construction. A triple extractor is developed, incorporating the rules. The extractor extracts triples (subject-relation-object) from the news articles dataset. These triples form the basis of this MKG based study of the authors.

\subsection{Language Model Approach}

A language model based approach by Ivanov et. al. for extracting requirements from unstructured text uses BERT \cite{devlin2018bert} model fine-tuned with a manually annotated dataset from the PURE (Public Requirements) corpus \cite{ferrari2017pure}. They achieved an accuracy of 74\%, compared to two other baseline approaches -- fastText (open source natural language processing toolkit) with an accuracy of 60\%, and ELMo (a foundational text-embedding model) paired with SVM (Support Vector Machine classifier) with an accuracy of 59\%. Due to the similarity of objective, this work was considered as a baseline for us to compare against. 


\section{Dataset}
\label{Dataset}

For our work the, the PURE (Public Requirements) dataset \cite{ferrari2017pure}, created by Ferrari et. al., was the primary source of data for the experiments of RO1. This contains 79 software requirements document (SRS) that contains 34,286 sentences. However, the dataset does not provide any additional labelling for the requirements sentences to aid natural language processing tasks. Some of the previous works, notably \cite{ivanov2021regexppure_extracting_software_requirements} and \cite{siahaan2022novel} utilized this dataset in their works via labeling in various ways. 

A labeled subset of the PURE dataset, called the RegExpPURE Dataset by Ivanov et. al. \cite{ivanov2021regexppure_extracting_software_requirements} was used in our studies. This dataset has proper balancing in it's data, with 2474 not requirements and 2832 requirements in train, 467 not requirements and 1058 requirements in test and 255 not requirements and 650 requirements in validation set, making it a suitable training candidate. Note that these same train, test, and validation sets were used to get a more comparable experiment setup to compare with the baseline work. 


\section{Methodology}
\label{Methodology}

From the discussions of possible approaches for NLP tasks concerned with primarily extraction and classification presented in \autoref{Literature Review}, it was decided to approach by leveraging transformers models. Note that much of our approach is uni-modal, only working with text based data. As of our current advancement and the nature of how requirements descriptions usually delivered to the development personnel and stakeholders, it is a reasonable assumption that utilizing text based data would be sufficient. The future extension of our work may be benefited from using a multimodal approach, similar to the works of Norabid et. al. \cite{norabid2022rule}.

\subsection{Choice of Language Models}

Here, we utilized two lightweight language models, based on the Transformers architecture - DistilBERT and RoBERTa. 

For natural languages, it is quite common that the correlation amongst the words in the context of the expression of that sentence can showcase long range dependencies. The self-attention mechanism allows Transformer models to capture long-range dependencies more effectively than RNNs and LSTMs, which struggle with long-term context due to their sequential nature \cite{vaswani2017attention_is_all_you_need}. Moreover the architecture of Transformer models is highly versatile and can be adapted to a wide range of NLP tasks, including the classification task in this work.

DistilBERT is a lightweight variant of the BERT model which retains almost 90\% of the BERT's performance in various NLP benchmarks \cite{sanh2019distilbert}. DistilBERT uses model compression (student-teacher approach) to reduce layers from BERT to half while matching the logits, hidden states and attention distribution. The reason why we chose this model for our work is the efficiency that DistilBERT showcases over BERT. DistilBERT requires lower memory, which makes it ideal for anyone to utilize this approach of this study without any significant computing requirements. On the other hand, RoBERTa is an optimized version of BERT that is trained on a larger text corpus than that of BERT and also uses dynamic masking during pre-training, which changes the masking pattern of the input sequence at each epoch, allowing the model to see a larger variety of contexts \cite{liu2019roberta}. Moreover, RoBERTa retains the same architecture as BERT but optimizes hyperparameters, including removing the Next Sentence Prediction (NSP) objective. This modification improves the efficiency of the training process and the model's overall performance \cite{liu2019roberta}. Due to these reasons, RoBRETa outperforms BERT outperforms BERT in many benchmarks and is a strong yet lightweight candidate for classification tasks. Hence, it was also chosen in our experiment. 

\subsection{Definition of Noise in Requirements}

In the context of software requirements, noise can be referred to as irrelevant information or non-essential details that do not contribute to the actual functionality or goals of the system. These noises can be redundant information, ambiguous language or non-requirements. To illustrate an example from the RegExpPURE dataset \cite{ivanov2021regexppure_extracting_software_requirements}, the following is a clear requirement:
\begin{quote}
    System Initialization shall [SRS014] initiate the watchdog timer.
\end{quote}
And the following is a noise:
\begin{quote}
    A Working Group was established in May, 2006 to develop high-level functional specifications for an NLM Digital Repository for NLM collection materials and to identify policy and management issues related to the creation, design and management of the repository.
\end{quote}
We can see that in order to recognize what is a requirement and what is not a requirement, one needs to read the entire sentence and map the terms of the sentences to clarify what the sentence expresses. This can be used as an analogy of what the term `long range dependency' entails for transformer models, the model's awareness of the expressive meaning of every term in the sequence with one another. 

\subsection{Ensemble Pooling Approach}

For this classification task of filtering noise, we propose a method that we call ``Ensemble Pooling''. Our approach consists of three layers - 
\begin{enumerate}
    \item \textbf{Tokenizer Layer :} This layer will appropriately tokenize the input sequence from text to corresponding vector representation.
    \item \textbf{Transformer Layer :} This layer will produce the contextual embedding for the input sequence by feeding into the vector representation of the tokenizer layer.
    \item \textbf{Classifier Layer :} This layer will use the contextual embedding (via pooling) produced by the transformer layer to classify the requirements from the noise. This layer would be trained with appropriate labels for classification. In our experimentation, we got better result with ensemble classifiers (hence the name, `Ensemble Pooling'). 
\end{enumerate}

The following \autoref{fig:ensemble pooling classification approach} represents the architecture of our approach.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{Figures/ensemble_pooling_classification.png}}
\caption{The Ensemble Pooling Classification Approach}
\label{fig:ensemble pooling classification approach}
\end{figure}

Here, we utilized the last hidden layer of the transformer models, which contains integrated information or the final contextualized embeddings from the entire sequence in a multidimensional vector representation (in the Huggingface Transformer library \cite{sanh2019distilbert}, the dimension is ($Seq_{size} \times Seq_{length} \times Size_{hidden}$) the default hidden size is $768$). But the most important content of the layer is the [CLS] token representation, which is added to each of the embeddings vector by the tokenizer and the vector for that token acts as the aggregate representation of the entire sequence \cite{devlin2018bert} \cite{sarzynska2021detecting_thought_contextualized}, that we use with the ensemble classifiers for classification tasks. The DistilBertForSequenceClassification and RobertaForSequenceClassification (based on DistilBERT and RoBERTa) classifiers offered by the Transformers library uses a similar approach, but with linear classification layer on top. These were compared as baselines.

Note that the transformer layer does not have any training cycle. This is because the transformer models used here (BERT and RoBERTa) are already trained over a large corpus of the English language and already have the necessary vocabulary structuring for language related tasks \cite{sanh2019distilbert} \cite{liu2019roberta}. This facilitates in reducing the cost and time that one had to invest with training. 

\subsection{Classification Layer}

The classification layers were trained over the pooled vectors from the transformer layer and the labels from the dataset. The labels were converted to 0 for not a requirement (i.e. noise) and 1 for requirement (i.e a proper requirement). Then 4 classifier models were trained and evaluated, which were Random Forest Classifier, XGB Classifier, LightGBM classifier and SVM Classifier. Out of these, 3 are ensemble models (Random Forest, XGB, LightGBM). We chose ensemble models because they leverage multiple learning algorithms (or sub-classifiers) to obtain better predictive performance than could be obtained from any of the standalone models. Moreover, due to combining sub-classifier, ensemble models are robust against overfitting \cite{breiman2001randomforest}. On the other hand, SVM is not an ensemble model, rather it is a standalone classifier that determines a hyperplane to best separate the datapoints into classes \cite{cortes1995svm}. SVM was also experimented with as it had shown good performance in NLP related classification tasks in several works \cite{drucker1999svm_spam} \cite{torisawa2007svm_ner} \cite{torisawa2007svm_ner}.

\subsection{Training Parameters}

For the experiments, the core DistilBERT and RoBERTa models, used for ensemble pooling were not trained specifically for this study. However the two transformer based classifiers and the other classifiers were trained. The transformer based classifier models were fine tuned for classification, with the hyperparameters being the same for both - Batch Size = 16, Max Length = 128, Learning Rate = $2\times 10^{-5}$ and was trained over 10 epochs. Adam optimizer was used and Cross Entropy Loss was the loss function for both of the models. Both DistilBERT and RoBERTa is based on the BERT model. Their parameters have no difference. Where they do differ is in how they handle sequences in their inner layers (which is discussed in \autoref{Methodology})

For the classifiers, all were fine tuned with 10 fold grid search cross validation. The target metric for the hyperparameter tuning was accuracy. The optimal hyperparameters for the models are as follows in \autoref{tab:lgbm_hyperparameters}, \autoref{tab:rf_hyperparameters}, \autoref{tab:svm_hyperparameters}, and \autoref{tab:xgb_hyperparameters}.

\begin{table}[htbp]
\caption{Random Forest Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{ } & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
Max Depth (\texttt{max\_depth})            & None & 10 \\
\hline
Min Samples Leaf (\texttt{min\_samples\_leaf})  & 4 & 4 \\
\hline
Min Samples Split (\texttt{min\_samples\_split}) & 2 & 2 \\
\hline
Number of Estimators (\texttt{n\_estimators})   & 300 & 100 \\
\hline
\end{tabular}
\label{tab:rf_hyperparameters}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{LightGBM Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{} & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
Colsample by Tree (\texttt{colsample\_bytree})   & 1.0 & 1.0 \\
\hline
Learning Rate (\texttt{learning\_rate})          & 0.1 & 0.1 \\
\hline
Min Child Samples (\texttt{min\_child\_samples}) & 20  & 50 \\
\hline
Number of Estimators (\texttt{n\_estimators})    & 300 & 200 \\
\hline
Num Leaves (\texttt{num\_leaves})                & 50  & 50 \\
\hline
Subsample (\texttt{subsample})                   & 0.8 & 0.8 \\
\hline
\end{tabular}
\label{tab:lgbm_hyperparameters}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{XGBoost Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{} & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
Learning Rate (\texttt{learning\_rate}) & 0.1 & 0.1 \\
\hline
Max Depth (\texttt{max\_depth}) & 5 & 5 \\
\hline
Number of Estimators (\texttt{n\_estimators}) & 300 & 300 \\
\hline
Subsample (\texttt{subsample}) & 0.8 & 0.7 \\
\hline
Colsample by Tree (\texttt{colsample\_bytree}) & 0.8 & 1 \\
\hline
Gamma (\texttt{gamma}) & 0.1 & 0 \\
\hline
\end{tabular}
\label{tab:xgb_hyperparameters}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{SVM Hyperparameters for RoBERTa and DistilBERT Pairings}
\begin{center}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Hyperparameter} & \multicolumn{2}{|c|}{\textbf{Model Pairing}} \\
\cline{2-3}
\textbf{} & \textbf{\textit{RoBERTa}} & \textbf{\textit{DistilBERT}} \\
\hline
C (\texttt{C}) & 10 & 10 \\
\hline
Gamma (\texttt{gamma}) & scale & auto \\
\hline
Kernel (\texttt{kernel}) & rbf & rbf \\
\hline
\end{tabular}
\label{tab:svm_hyperparameters}
\end{center}
\end{table}

Again, the term ``Pairing'' is referred to as using the hidden layer of the language models through the classifier models to make predictions. 


\section{Results}
\label{Results}

As per the setup described in \autoref{Methodology}, we compared our experiments with the baseline paper. Similar to the baseline work, results are based on the validation set. For the result comparison, primarily used the accuracy metric (The rationale behind this choice of metric is discussed in the \autoref{Discussion}). The \autoref{table:results_comparison} contains the results and the \autoref{table:baseline_results} shows the baseline paper result.  

\begin{table}[htbp]
\caption{Results Comparison of the Various Approaches}
\begin{center}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Approach Category} & \textbf{Model / Technique} & \textbf{Accuracy (\%)} \\
\hline
\multirow{5}{*}{\textbf{\textit{DistilBERT}}}
 & DistilBERT Sequence Classifier & 76 \\
 & DistilBERT + Random Forest     & \textbf{78} \\
 & DistilBERT + XGB               & 77 \\
 & DistilBERT + LightGBM          & 77 \\
 & DistilBERT + SVM               & 76 \\
\hline
\multirow{5}{*}{\textbf{\textit{RoBERTa}}}
 & RoBERTa Sequence Classifier    & 77 \\
 & RoBERTa + Random Forest        & 75 \\
 & RoBERTa + XGB                  & 77 \\
 & RoBERTa + LightGBM             & 77 \\
 & RoBERTa + SVM                  & 77 \\
\hline
\end{tabular}
\label{table:results_comparison}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Results of the Baseline Paper \cite{ivanov2021regexppure_extracting_software_requirements}}
\begin{center}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|c|c|}
\hline
\textbf{Approach} & \textbf{Accuracy (\%)} \\
\hline
fastText   & 60 \\
\hline
ELMo + SVM & 59 \\
\hline
BERT       & 74 \\
\hline
\end{tabular}
\label{table:baseline_results}
\end{center}
\end{table}

For our experiments and proposed approach of identifying requirement noise, we used the RegExpPURE dataset \cite{ivanov2021regexppure_extracting_software_requirements}. So, our baseline was the results in that paper for the dataset in which they utilized the BERT base model for extracting requirements. Though their work may not directly state noise reduction, but their work achieved similar objectives. As seen in \autoref{Dataset} their dataset consists labeled sentences of requirements and non requirements. And these non requirement sentences are precisely what we have defined as noise. So, the rationale for using this paper as a baseline is quite reasonable.


\section{Discussion}
\label{Discussion}

\subsection{The Rationale for Accuracy}

Accuracy is intuitive and easy to understand. It directly reflects the overall correctness of the model’s predictions. In many scenarios, balanced performance across different classes is desirable. Accuracy treats all classes equally. Moreover, when the class distribution is roughly uniform (which is the case for the RegExpPURE Dataset), accuracy is a reasonable choice. And hence, accuracy was the chosen metric. \cite{sokolova2009systematic_classification}.

\subsection{Base Classifier vs Ensemble Pooling}

From the results in \autoref{table:results_comparison} and \autoref{table:baseline_results} shows that our approach combining DistilBERT and Random Forest in ensemble pooling performs better in terms of accuracy when compared against all the approaches and baseline results. However, all the approaches with RoBERTa had quite close results, except Random Forest. This deviation is most likely due to how RoBERTa utilizes byte-pair encoding in tokenization and how the inner encoder layers attune to the context of the sequences \cite{liu2019roberta}. 

The baseline paper showed the most promising result with BERT with about 74\% in accuracy. But baseline classifier of BERT, similar to DistilBERT and RoBERTa uses a neural network layer with a linear softmax activation function to generate class probability distribution. While effective in most cases, this classification task can benefit from more robust method of classification, ensemble classification. Since ensemble classifiers use bagging of sub classifiers, it is more robust in classification. Random Forest is one such ensemble method that ensembles decision trees. These trees or sub classifiers work in tandem for classification and improves accuracy, as seen from our experiments. Again, the rationale behind choosing DistilBERT and RoBERTa comes to their lightweight performance. They require much less computing resources compared to some other larger models and still remains a popular choice for smaller NLP related works. 

However, the lightweight nature is also one of the drawbacks of our approach. The accuracy results are still in the range below 80, which is a clear indication that these models fall short due to having smaller number of parameters compared to state of the art GPT, Gemini and LLAMA models (inferred from the ``Densing law of LLMs'' \cite{xiao2025densing}). So, as a future extension in requirement noise reduction, the viability of the state of the art models as well as stronger classification layers (stacking multiple classifiers for starters) remain as an yet to explore prospect. 


\section{Conclusion}
\label{Conclusion}

In conclusion, this study contributed valuable insights into the application of transformer models in natural language processing tasks, specifically in the context of software requirements classification. By focusing on noise reduction, this research lays the foundation for enhancing the efficiency and effectiveness of the software development process. Our developed methodology showcased improved performance when compared with various out-of-the-box classifiers in terms of accuracy.

While the current work primarily addressed noise reduction in requirements, the future direction of this research involves extending the classification pipeline to encompass all research objectives, including improving requirement assessment, conflict resolution and multimodal analysis (involving charts, diagrams, wireframes - etc.). 



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
